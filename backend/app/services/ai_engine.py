import os
import cv2
import numpy as np
import logging
from ultralytics import YOLO
from insightface.app import FaceAnalysis

# ======================================
# ğŸ”¹ ë¡œê¹… ì„¤ì •
# ======================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ======================================
# ğŸ”¹ ì „ì—­ ë³€ìˆ˜ ë° ì„¤ì •
# ======================================
model = None
face_app = None

BLUR_MODE = 'mosaic'      # 'gaussian', 'box', 'bilateral', 'mosaic'
FEATHER_PX = 6            # ê²½ê³„ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬
FACE_PAD_RATIO = 0.18     # ì–¼êµ´ ì˜ì—­ í™•ì¥ ë¹„ìœ¨
FALLBACK_TO_PERSON_MASK = True  # ì–¼êµ´ ë¯¸ê²€ì¶œ ì‹œ ì „ì‹  ë¸”ëŸ¬ í´ë°±


# ======================================
# ğŸ”¹ ëª¨ë¸ ë¡œë“œ
# ======================================
def load_model():
    global model, face_app

    if model is not None and face_app is not None:
        logger.info("âœ… ëª¨ë¸ë“¤ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return True

    try:
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        MODEL_PATH = os.path.join(BASE_DIR, "models", "best.pt")

        if not os.path.exists(MODEL_PATH):
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {MODEL_PATH}")
            return False

        model = YOLO(MODEL_PATH, task="segment")
        logger.info(f"âœ… YOLO ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {MODEL_PATH}")
        logger.info(f"í´ë˜ìŠ¤ ì´ë¦„: {model.names}")

        # ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ
        face_app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
        face_app.prepare(ctx_id=0, det_size=(640, 640))
        logger.info("âœ… ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")

        return True

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


# ======================================
# ğŸ”¹ ìœ í‹¸ í•¨ìˆ˜
# ======================================
def clamp(val, lo, hi):
    return max(lo, min(hi, val))

def expand_box(x1, y1, x2, y2, pad_ratio, W, H):
    w = x2 - x1
    h = y2 - y1
    px = int(round(w * pad_ratio))
    py = int(round(h * pad_ratio))
    ex1 = clamp(x1 - px, 0, W - 1)
    ey1 = clamp(y1 - py, 0, H - 1)
    ex2 = clamp(x2 + px, 0, W - 1)
    ey2 = clamp(y2 + py, 0, H - 1)
    return ex1, ey1, ex2, ey2

def adaptive_kernel(w, h, frac=0.15, kmin=9, kmax=91):
    k = int(round(min(w, h) * frac))
    if k % 2 == 0:
        k += 1
    return clamp(k, kmin, kmax)

def build_alpha_from_mask(mask_uint8, feather_px=6):
    if feather_px <= 0:
        return (mask_uint8 / 255.0).astype(np.float32)

    dist = cv2.distanceTransform(255 - mask_uint8, cv2.DIST_L2, 3)
    edge = np.clip(dist / float(feather_px), 0, 1)
    alpha = (mask_uint8 / 255.0) * (1 - edge)
    return np.clip(alpha, 0, 1).astype(np.float32)


# ======================================
# ğŸ”¹ ë¸”ëŸ¬ í•¨ìˆ˜
# ======================================
def apply_blur_with_alpha(img, mask_uint8, blur_mode='mosaic', feather_px=6, bbox_hint=None):
    H, W = img.shape[:2]
    alpha = build_alpha_from_mask(mask_uint8, feather_px)
    alpha3 = alpha[..., None]

    # ì»¤ë„ í¬ê¸°
    if bbox_hint:
        x1, y1, x2, y2 = bbox_hint
        k = adaptive_kernel(x2 - x1, y2 - y1, 0.15)
    else:
        k = 25
        if k % 2 == 0:
            k += 1

    # ë¸”ëŸ¬ ë°©ì‹
    if blur_mode == 'gaussian':
        blurred = cv2.GaussianBlur(img, (k, k), 0)
    elif blur_mode == 'box':
        blurred = cv2.blur(img, (k, k))
    elif blur_mode == 'bilateral':
        blurred = cv2.bilateralFilter(img, 9, 75, 75)
    elif blur_mode == 'mosaic':
        cell = max(8, int(round(k * 0.6)))
        small = cv2.resize(img, (max(1, W // cell), max(1, H // cell)), interpolation=cv2.INTER_LINEAR)
        blurred = cv2.resize(small, (W, H), interpolation=cv2.INTER_NEAREST)
    else:
        blurred = img.copy()

    out = (alpha3 * blurred + (1 - alpha3) * img).astype(np.uint8)
    return out


def mask_from_polygon_or_bbox(mask_shape, bbox=None, ellipse=False):
    mask = np.zeros(mask_shape[:2], dtype=np.uint8)
    if bbox is not None:
        x1, y1, x2, y2 = map(int, bbox)
        if ellipse:
            cx = (x1 + x2)//2
            cy = (y1 + y2)//2
            ax = (x2 - x1)//2
            ay = (y2 - y1)//2
            cv2.ellipse(mask, (cx, cy), (ax, ay), 0, 0, 360, 255, -1)
        else:
            cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)
    return mask


# ======================================
# ğŸ”¹ ì‚¬ëŒ ë° ì°¨ëŸ‰ ì²˜ë¦¬
# ======================================
def process_person_with_face_detection(img, out, x1, y1, x2, y2, masks, i, blur_mode):
    global face_app
    H, W = out.shape[:2]
    rx1, ry1, rx2, ry2 = expand_box(x1, y1, x2, y2, pad_ratio=0.02, W=W, H=H)
    roi = out[ry1:ry2, rx1:rx2]

    try:
        faces = face_app.get(roi)
    except Exception as e:
        logger.warning(f"ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {e}")
        faces = []

    face_found = False
    for f in faces:
        fx1, fy1, fx2, fy2 = f.bbox.astype(int)
        fx1, fy1, fx2, fy2 = fx1 + rx1, fy1 + ry1, fx2 + rx1, fy2 + ry1
        fx1, fy1, fx2, fy2 = expand_box(fx1, fy1, fx2, fy2, FACE_PAD_RATIO, W, H)
        face_mask = mask_from_polygon_or_bbox(out.shape, bbox=(fx1, fy1, fx2, fy2), ellipse=True)
        out = apply_blur_with_alpha(out, face_mask, blur_mode=blur_mode, feather_px=FEATHER_PX, bbox_hint=(fx1, fy1, fx2, fy2))
        face_found = True
        logger.info(f"[FACE] ì–¼êµ´ ë¸”ëŸ¬ ì ìš©: ({fx1}, {fy1}, {fx2}, {fy2})")

    if not face_found and FALLBACK_TO_PERSON_MASK:
        logger.info("[FACE] ì–¼êµ´ ì—†ìŒ â†’ ì „ì‹  ë¸”ëŸ¬ í´ë°± ì ìš©")
        if masks is not None:
            m = (masks[i] * 255).astype(np.uint8)
            m = cv2.resize(m, (W, H), interpolation=cv2.INTER_NEAREST)
        else:
            m = mask_from_polygon_or_bbox(out.shape, bbox=(x1, y1, x2, y2))
        out = apply_blur_with_alpha(out, m, blur_mode=blur_mode, feather_px=FEATHER_PX, bbox_hint=(x1, y1, x2, y2))
        logger.info("[FACE] ì „ì‹  ë¸”ëŸ¬ í´ë°± ì™„ë£Œ")

    return out


def process_vehicle(out, x1, y1, x2, y2, masks, i, blur_mode):
    H, W = out.shape[:2]
    if masks is not None:
        m = (masks[i] * 255).astype(np.uint8)
        m = cv2.resize(m, (W, H), interpolation=cv2.INTER_NEAREST)
    else:
        m = mask_from_polygon_or_bbox(out.shape, bbox=(x1, y1, x2, y2))
    out = apply_blur_with_alpha(out, m, blur_mode=blur_mode, feather_px=FEATHER_PX, bbox_hint=(x1, y1, x2, y2))
    logger.info(f"[VEHICLE] ë¸”ëŸ¬ ì ìš©: ({x1}, {y1}, {x2}, {y2})")
    return out


# ======================================
# ğŸ”¹ ì´ë¯¸ì§€ ì²˜ë¦¬ (í•µì‹¬ ìˆ˜ì •ë¨)
# ======================================
def process_image_advanced(image_input, blur_mode=BLUR_MODE):
    global model, face_app

    if model is None or face_app is None:
        logger.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    if isinstance(image_input, str):
        img = cv2.imread(image_input)
        if img is None:
            logger.error(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_input}")
            return None
    elif isinstance(image_input, np.ndarray):
        img = image_input
    else:
        logger.error("ì˜ëª»ëœ ì´ë¯¸ì§€ ì…ë ¥ íƒ€ì…ì…ë‹ˆë‹¤.")
        return None

    results = model(img, verbose=False)[0]
    out = img.copy()

    masks = results.masks.data.cpu().numpy() if results.masks is not None else None
    boxes = results.boxes

    if boxes is None or len(boxes) == 0:
        logger.info("íƒì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return out

    cls_ids = boxes.cls.cpu().numpy().astype(int)
    logger.info(f"ê°ì§€ëœ í´ë˜ìŠ¤ ëª©ë¡: {cls_ids}")

    for i, cls_id in enumerate(cls_ids):
        x1, y1, x2, y2 = boxes.xyxy[i].cpu().numpy().astype(int)
        confidence = float(boxes.conf[i].cpu().numpy())

        logger.info(f"[YOLO] ê°ì²´ {i}: í´ë˜ìŠ¤={cls_id}, ì‹ ë¢°ë„={confidence:.2f}, ì¢Œí‘œ=({x1},{y1},{x2},{y2})")

        if confidence < 0.3:
            continue

        # ğŸ”¸ í˜„ì¬ ëª¨ë¸ì€ 0=person, 1=vehicle êµ¬ì¡°
        if cls_id == 0:
            out = process_person_with_face_detection(img, out, x1, y1, x2, y2, masks, i, blur_mode)
        elif cls_id == 1:
            out = process_vehicle(out, x1, y1, x2, y2, masks, i, blur_mode)

    return out


# ======================================
# ğŸ”¹ analyze (FastAPIìš©) â€” SSE ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ê°œì„  ë²„ì „
# ======================================
from app.services.state import PROCESS_STATUS, PROCESS_LOCK

def analyze(frame_files, file_id, chunk_idx=None, total_chunks=None, blur_mode=BLUR_MODE):
    """ê° í”„ë ˆì„ ë‹¨ìœ„ ë° ë‚´ë¶€ ê°ì²´ ì²˜ë¦¬ ë‹¨ìœ„ë¡œ ì§„í–‰ë¥ ì„ ê°±ì‹ í•˜ëŠ” ê°œì„ ëœ analyze í•¨ìˆ˜"""
    from time import time

    if model is None or face_app is None:
        logger.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {"error": "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "images": [], "total_detections": 0}

    result_dir = os.path.join("./results", f"{file_id}_{chunk_idx or 0}")
    os.makedirs(result_dir, exist_ok=True)

    total_frames = len(frame_files)
    processed_images = []
    total_detections = 0

    start_time = time()
    logger.info(f"[ë¶„ì„ ì‹œì‘] file_id={file_id}, chunk={chunk_idx}, ì´ {total_frames} í”„ë ˆì„")

    for i, frame_path in enumerate(frame_files):
        if not os.path.exists(frame_path):
            logger.warning(f"âš ï¸ í”„ë ˆì„ ì—†ìŒ: {frame_path}")
            continue

        img = cv2.imread(frame_path)
        if img is None:
            continue

        # YOLO íƒì§€
        results = model(img, verbose=False)[0]
        out = img.copy()
        masks = results.masks.data.cpu().numpy() if results.masks is not None else None
        boxes = results.boxes

        # ê°ì²´ íƒì§€ ì—†ëŠ” ê²½ìš°
        if boxes is None or len(boxes) == 0:
            cv2.imwrite(os.path.join(result_dir, f"processed_frame_{i:04d}.jpg"), out)
            continue

        cls_ids = boxes.cls.cpu().numpy().astype(int)
        for j, cls_id in enumerate(cls_ids):
            x1, y1, x2, y2 = boxes.xyxy[j].cpu().numpy().astype(int)
            conf = float(boxes.conf[j].cpu().numpy())
            if conf < 0.3:
                continue

            # --- ê°ì²´ë³„ ë¶„ê¸° ---
            if cls_id == 0:
                out = process_person_with_face_detection(img, out, x1, y1, x2, y2, masks, j, blur_mode)
            elif cls_id == 1:
                out = process_vehicle(out, x1, y1, x2, y2, masks, j, blur_mode)

            total_detections += 1

            # ğŸ”¸ (1) ê°ì²´ë³„ ë¶€ë¶„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ë” ë¶€ë“œëŸ¬ìš´ SSE í‘œì‹œìš©)
            if file_id in PROCESS_STATUS and chunk_idx is not None and total_chunks is not None:
                with PROCESS_LOCK:
                    # í•œ í”„ë ˆì„ ë‚´ ê°ì²´ ë¹„ìœ¨ ë°˜ì˜ (ì˜ˆ: 0.1%)
                    partial = ((i + j / len(cls_ids)) / total_frames) * 100
                    PROCESS_STATUS[file_id]["chunks"][chunk_idx] = partial
                    avg = sum(PROCESS_STATUS[file_id]["chunks"]) / total_chunks
                    PROCESS_STATUS[file_id]["progress"] = round(10 + avg * 0.85, 2)
                    PROCESS_STATUS[file_id]["stage"] = (
                        f"ì²­í¬ {chunk_idx+1}/{total_chunks} - í”„ë ˆì„ {i+1}/{total_frames}"
                    )

        # --- í”„ë ˆì„ ì €ì¥ ---
        output_path = os.path.join(result_dir, f"processed_frame_{i:04d}.jpg")
        cv2.imwrite(output_path, out)
        processed_images.append(output_path)

        # ğŸ”¸ (2) í”„ë ˆì„ ë‹¨ìœ„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if file_id in PROCESS_STATUS and chunk_idx is not None and total_chunks is not None:
            local_progress = ((i + 1) / total_frames) * 100
            with PROCESS_LOCK:
                PROCESS_STATUS[file_id]["chunks"][chunk_idx] = local_progress
                avg_progress = sum(PROCESS_STATUS[file_id]["chunks"]) / total_chunks
                PROCESS_STATUS[file_id]["progress"] = round(10 + avg_progress * 0.85, 2)
                PROCESS_STATUS[file_id]["stage"] = (
                    f"ì²­í¬ {chunk_idx+1}/{total_chunks} - í”„ë ˆì„ {i+1}/{total_frames}"
                )
                logger.info(f"[ì§„í–‰ë¥ ] {file_id}: {PROCESS_STATUS[file_id]['progress']}%")

    elapsed = round(time() - start_time, 2)
    logger.info(f"[âœ… ì™„ë£Œ] file_id={file_id}, chunk={chunk_idx}, {total_detections}ê°œ íƒì§€, {elapsed}s ì†Œìš”")

    return {
        "status": "success",
        "images": processed_images,
        "total_detections": total_detections
    }


# ======================================
# ğŸ”¹ ëª¨ë¸ ìë™ ë¡œë“œ
# ======================================
load_model()
